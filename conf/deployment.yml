# JOB API V2.1
# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "11.3.x-cpu-ml-scala2.12"
    num_workers: 1
    # node_type_id: "Standard_E8_v3"
    # init_scripts:
    #   - dbfs:
    #       destination: "dbfs:/databricks/scripts/external-metastore.sh"    

  dev-pool-cluster: &dev-pool-cluster
    new_cluster:
      <<: *basic-cluster-props
      instance_pool_id: "1027-141550-dogs36-pool-qq92uldu"

  staging-pool-cluster: &staging-pool-cluster
    new_cluster:
      <<: *basic-cluster-props
      instance_pool_id: "0520-103527-swamp3-pool-8kkbt5ma"

  prod-pool-cluster: &prod-pool-cluster
    new_cluster:
      <<: *basic-cluster-props
      instance_pool_id: "0520-103936-jokes3-pool-46dtg4oi"            


environments:
  default:
    workflows:

      #############################################################
      # ETL workflow
      #############################################################
      - name: "etl-workflow"
        job_clusters:
          - job_cluster_key: "default"
            <<: *dev-pool-cluster
        schedule:
          quartz_cron_expression: 0 0 0/2 * * ? # every 2 hours
          timezone_id: Europe/Kiev
          pause_status: UNPAUSED              
        tasks:
          - task_key: "step-etl-1-task"
            description: "Step etl-1: data generation"
            job_cluster_key: "default"
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/etl_1_data_generation.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/etl_1_data_generation.yml" ]      
          - task_key: "step-etl-2-task"
            depends_on:
              - task_key: "step-etl-1-task"          
            description: "Step etl-2: features generation"
            job_cluster_key: "default"
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/etl_2_feature_generation.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/etl_2_feature_generation.yml" ]                 

      #############################################################
      # Training workflow
      #############################################################
      # DEV
      - name: "train-workflow-dev"
        job_clusters:
          - job_cluster_key: "default"
            <<: *dev-pool-cluster          
        tasks:
          - task_key: "step-training-task"
            description: "Step Training"
            job_cluster_key: "default"
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_training.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_training_dev.yml" ]
          - task_key: "step-validation-task"
            depends_on:
              - task_key: "step-training-task"
            job_cluster_key: "default"
            max_retries: 0            
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_validation.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_validation_dev.yml" ]

      # STAGING
      - name: "train-workflow-staging"
        job_clusters:
          - job_cluster_key: "default"
            <<: *staging-pool-cluster   
        schedule:
          quartz_cron_expression: 0 0 0 * * ? # every day at midnight
          timezone_id: Europe/Kiev
          pause_status: UNPAUSED                      
        tasks:
          - task_key: "step-training-task"
            description: "Step Training"
            job_cluster_key: "default"
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_training.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_training_staging.yml" ]
          - task_key: "step-validation-task"
            depends_on:
              - task_key: "step-training-task"
            job_cluster_key: "default"
            max_retries: 0            
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_validation.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_validation_staging.yml" ]              

      #############################################################
      # Inference workflow
      #############################################################
      - name: "inference-workflow-dev"
        job_clusters:
          - job_cluster_key: "default"
            <<: *dev-pool-cluster
        tasks:
          - task_key: "step-infer-task"
            description: "Step Inference Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/step_inference.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/model.yml" ]         

      - name: "inference-workflow-uat"
        job_clusters:
          - job_cluster_key: "default"
            <<: *staging-pool-cluster
        tasks:
          - task_key: "step-infer-task"
            description: "Step Inference Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/step_inference.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/model.yml" ]            

      - name: "inference-workflow-prod"
        job_clusters:
          - job_cluster_key: "default"
            <<: *prod-pool-cluster
        tasks:
          - task_key: step-transition-to-prod
            description: step transition to prod task
            job_cluster_key: "default"
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/step_transition_to_prod.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/model.yml" ]              

          - task_key: "step-infer-task"
            depends_on:
              - task_key: "step-transition-to-prod"          
            description: "Step Inference Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/step_inference.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/model.yml" ]                               